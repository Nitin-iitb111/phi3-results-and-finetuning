{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Phi-3 Vision Analysis Pipeline for Google Colab\n",
        "Handles benchmark replication, domain adaptation, and tuning strategies\n",
        "Compatible with Colab's GPU limitations (no flash_attn2)\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Environment Setup and Installation\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip install -q datasets\n",
        "!pip install -q pillow\n",
        "!pip install -q torch\n",
        "!pip install -q peft\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q evaluate\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q tqdm\n",
        "!pip install -q pycocotools\n",
        "!pip install -q nltk\n",
        "!pip install -q rouge-score\n",
        "!pip install -q bert-score\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsjKUflMVPQZ",
        "outputId": "3203e38d-bda7-4cb6-a84b-8027fce1150b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n"
      ],
      "metadata": {
        "id": "io16Y_0lVT-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Check GPU\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WjWOA0VVWKw",
        "outputId": "b45c550d-75fa-4994-e359-60c5c2ee4895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Configuration Classes\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for Phi-3 Vision model\"\"\"\n",
        "    model_name: str = \"microsoft/Phi-3-vision-128k-instruct\"\n",
        "    use_quantization: bool = True\n",
        "    quantization_bits: int = 4\n",
        "    device_map: str = \"auto\"\n",
        "    # Use float32 by default to avoid fp16 mask underflow causing device asserts\n",
        "    torch_dtype: torch.dtype = torch.float32\n",
        "    trust_remote_code: bool = True\n",
        "    attn_implementation: str = \"eager\"  # Colab-compatible\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FineTuningConfig:\n",
        "    \"\"\"Configuration for feature-based fine-tuning\"\"\"\n",
        "    # Training hyperparameters\n",
        "    num_epochs: int = 5\n",
        "    batch_size: int = 2\n",
        "    learning_rate: float = 1e-4\n",
        "    warmup_steps: int = 100\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    max_grad_norm: float = 1.0\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # Feature adapter configuration\n",
        "    adapter_hidden_dim: int = 256\n",
        "    adapter_dropout: float = 0.1\n",
        "    adapter_bottleneck_dim: int = 64\n",
        "    num_adapter_layers: int = 2\n",
        "\n",
        "    # Output directories\n",
        "    output_dir: str = \"./phi3_finetuning_outputs\"\n",
        "    checkpoints_dir: str = \"./phi3_finetuning_outputs/checkpoints\"\n",
        "    visualizations_dir: str = \"./phi3_finetuning_outputs/visualizations\"\n",
        "\n",
        "    # Evaluation settings\n",
        "    eval_steps: int = 50\n",
        "    save_steps: int = 100\n",
        "    logging_steps: int = 10\n",
        "    max_samples_train: Optional[int] = 50\n",
        "    max_samples_eval: Optional[int] = 10\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Feature Adapter Architecture\n",
        "# ============================================================================\n",
        "\n",
        "class FeatureAdapter(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature adapter layer inspired by CLIP-Adapter\n",
        "    Adds lightweight trainable layers to frozen vision-language features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int = 256,\n",
        "        bottleneck_dim: int = 64,\n",
        "        dropout: float = 0.1,\n",
        "        num_layers: int = 2\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bottleneck_dim = bottleneck_dim\n",
        "\n",
        "        # Bottleneck architecture for efficiency\n",
        "        layers = []\n",
        "\n",
        "        # Down-projection\n",
        "        layers.append(nn.Linear(input_dim, bottleneck_dim))\n",
        "        layers.append(nn.LayerNorm(bottleneck_dim))\n",
        "        layers.append(nn.GELU())\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        # Middle layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.append(nn.Linear(bottleneck_dim, bottleneck_dim))\n",
        "            layers.append(nn.LayerNorm(bottleneck_dim))\n",
        "            layers.append(nn.GELU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        # Up-projection\n",
        "        layers.append(nn.Linear(bottleneck_dim, input_dim))\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "        # Learnable scaling factor (alpha)\n",
        "        self.alpha = nn.Parameter(torch.ones(1) * 0.1)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize adapter weights\"\"\"\n",
        "        for module in self.adapter.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with residual connection\n",
        "        x: input features [batch_size, seq_len, hidden_dim]\n",
        "        \"\"\"\n",
        "        # Adapter transformation\n",
        "        adapter_output = self.adapter(x)\n",
        "\n",
        "        # Residual connection with learnable scaling\n",
        "        output = x + self.alpha * adapter_output\n",
        "\n",
        "        return output\n",
        "\n",
        "class Phi3WithFeatureAdapters(nn.Module):\n",
        "    \"\"\"\n",
        "    Phi-3 Vision model with feature adapters\n",
        "    Adds adapters to vision and language feature spaces\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, config: FineTuningConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.config = config\n",
        "\n",
        "        # Freeze base model\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get hidden dimension from model config\n",
        "        self.hidden_dim = base_model.config.hidden_size\n",
        "\n",
        "        # Create feature adapters for different layers\n",
        "        self.vision_adapter = FeatureAdapter(\n",
        "            input_dim=self.hidden_dim,\n",
        "            hidden_dim=config.adapter_hidden_dim,\n",
        "            bottleneck_dim=config.adapter_bottleneck_dim,\n",
        "            dropout=config.adapter_dropout,\n",
        "            num_layers=config.num_adapter_layers\n",
        "        )\n",
        "\n",
        "        self.language_adapter = FeatureAdapter(\n",
        "            input_dim=self.hidden_dim,\n",
        "            hidden_dim=config.adapter_hidden_dim,\n",
        "            bottleneck_dim=config.adapter_bottleneck_dim,\n",
        "            dropout=config.adapter_dropout,\n",
        "            num_layers=config.num_adapter_layers\n",
        "        )\n",
        "\n",
        "        # Cross-modal fusion adapter\n",
        "        self.fusion_adapter = FeatureAdapter(\n",
        "            input_dim=self.hidden_dim,\n",
        "            hidden_dim=config.adapter_hidden_dim,\n",
        "            bottleneck_dim=config.adapter_bottleneck_dim,\n",
        "            dropout=config.adapter_dropout,\n",
        "            num_layers=config.num_adapter_layers\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✓ Feature adapters initialized\")\n",
        "        print(f\"  - Vision adapter: {sum(p.numel() for p in self.vision_adapter.parameters())} params\")\n",
        "        print(f\"  - Language adapter: {sum(p.numel() for p in self.language_adapter.parameters())} params\")\n",
        "        print(f\"  - Fusion adapter: {sum(p.numel() for p in self.fusion_adapter.parameters())} params\")\n",
        "        print(f\"  - Total trainable: {self.count_trainable_parameters()} params\")\n",
        "\n",
        "    def count_trainable_parameters(self):\n",
        "        \"\"\"Count trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "      \"\"\"Forward pass with adapter integration\"\"\"\n",
        "      # Request hidden states from base model\n",
        "      kwargs['output_hidden_states'] = True\n",
        "      outputs = self.base_model(**kwargs)\n",
        "\n",
        "      # Determine hidden states: prefer .last_hidden_state (common name) or hidden_states[-1]\n",
        "      if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:\n",
        "          hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
        "      else:\n",
        "          # fallback to transformer hidden states tuple\n",
        "          hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "      # Ensure dtype for adapter computation is safe (use float32)\n",
        "      adapter_input = hidden_states\n",
        "      if adapter_input.dtype != torch.float32:\n",
        "          adapter_input = adapter_input.to(torch.float32)\n",
        "\n",
        "      adapted_hidden_states = self.fusion_adapter(adapter_input)\n",
        "\n",
        "      # Cast back to original dtype if needed\n",
        "      if adapted_hidden_states.dtype != hidden_states.dtype:\n",
        "          adapted_hidden_states = adapted_hidden_states.to(hidden_states.dtype)\n",
        "\n",
        "      # Place adapted hidden states into outputs so loss/generation uses them\n",
        "      try:\n",
        "          outputs.last_hidden_state = adapted_hidden_states\n",
        "      except Exception:\n",
        "          # If attribute not writable, create a new output object by copying dict\n",
        "          out_dict = outputs.to_dict()\n",
        "          out_dict['last_hidden_state'] = adapted_hidden_states\n",
        "          # recreate a BaseModelOutput-like object using the model's output class if possible\n",
        "          # (best-effort; many HuggingFace outputs accept direct construction)\n",
        "          from transformers.modeling_outputs import BaseModelOutput\n",
        "          outputs = BaseModelOutput(**out_dict)\n",
        "\n",
        "      # also fix hidden_states tuple if present\n",
        "      if hasattr(outputs, 'hidden_states') and outputs.hidden_states:\n",
        "          hs_list = list(outputs.hidden_states)\n",
        "          hs_list[-1] = adapted_hidden_states\n",
        "          outputs.hidden_states = tuple(hs_list)\n",
        "\n",
        "      adapted_hidden_states = self.fusion_adapter(hidden_states)\n",
        "\n",
        "      # Replace last hidden state\n",
        "      outputs.hidden_states = list(outputs.hidden_states)\n",
        "      outputs.hidden_states[-1] = adapted_hidden_states\n",
        "\n",
        "      return outputs\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self, **kwargs):\n",
        "        \"\"\"Generation method for inference\"\"\"\n",
        "        return self.base_model.generate(**kwargs)\n",
        "\n",
        "    def save_adapters(self, save_path: str):\n",
        "        \"\"\"Save only adapter weights\"\"\"\n",
        "        adapter_state = {\n",
        "            'vision_adapter': self.vision_adapter.state_dict(),\n",
        "            'language_adapter': self.language_adapter.state_dict(),\n",
        "            'fusion_adapter': self.fusion_adapter.state_dict(),\n",
        "        }\n",
        "        torch.save(adapter_state, save_path)\n",
        "        print(f\"Adapters saved to: {save_path}\")\n",
        "\n",
        "    def load_adapters(self, load_path: str):\n",
        "        \"\"\"Load adapter weights\"\"\"\n",
        "        adapter_state = torch.load(load_path)\n",
        "        self.vision_adapter.load_state_dict(adapter_state['vision_adapter'])\n",
        "        self.language_adapter.load_state_dict(adapter_state['language_adapter'])\n",
        "        self.fusion_adapter.load_state_dict(adapter_state['fusion_adapter'])\n",
        "        print(f\"Adapters loaded from: {load_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aG-w8lT3VuvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: Model Loader (Reusing from original code)\n",
        "# ============================================================================\n",
        "\n",
        "class Phi3VisionLoader:\n",
        "    \"\"\"Handles model loading with Colab-specific configurations\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model_and_processor(config: ModelConfig):\n",
        "        \"\"\"Load Phi-3 Vision with Colab-compatible settings\"\"\"\n",
        "        print(\"Loading Phi-3 Vision model...\")\n",
        "\n",
        "        quantization_config = None\n",
        "        if config.use_quantization:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "              load_in_4bit=True,\n",
        "              bnb_4bit_quant_type=\"nf4\",\n",
        "              bnb_4bit_compute_dtype=torch.float32,\n",
        "              bnb_4bit_use_double_quant=True,\n",
        "              llm_int8_enable_fp32_cpu_offload=True, # Added to allow CPU offload\n",
        "          )\n",
        "\n",
        "\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=config.trust_remote_code\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.model_name,\n",
        "            device_map=config.device_map,\n",
        "            torch_dtype=config.torch_dtype,\n",
        "            trust_remote_code=config.trust_remote_code,\n",
        "            quantization_config=quantization_config,\n",
        "            attn_implementation=config.attn_implementation,\n",
        "            _attn_implementation=config.attn_implementation\n",
        "        )\n",
        "\n",
        "        model.config.use_cache = False\n",
        "\n",
        "        print(f\"✓ Model loaded on {next(model.parameters()).device}\")\n",
        "        print(f\"✓ Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
        "\n",
        "        return model, processor\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Dataset Handlers (Reusing from original code)\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetHandler:\n",
        "    \"\"\"Unified dataset handler for training\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_training_dataset(dataset_name: str = \"DocVQA\", max_samples: Optional[int] = None):\n",
        "        \"\"\"Load dataset for fine-tuning\"\"\"\n",
        "        print(f\"Loading {dataset_name} dataset for training...\")\n",
        "\n",
        "        if dataset_name == \"DocVQA\":\n",
        "            dataset = load_dataset(\"nielsr/docvqa_1200_examples_donut\", split=\"train\")\n",
        "        elif dataset_name == \"ScienceQA\":\n",
        "            dataset = load_dataset(\"derek-thomas/ScienceQA\", split=\"train\")\n",
        "            dataset = dataset.filter(lambda x: x['image'] is not None)\n",
        "        elif dataset_name == \"COCO\":\n",
        "            dataset = load_dataset(\"HuggingFaceM4/COCO\", split=\"train\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "        if max_samples:\n",
        "            dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "        print(f\"✓ Loaded {len(dataset)} training samples\")\n",
        "        return dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_sample(sample, processor, dataset_name: str):\n",
        "        \"\"\"Prepare sample based on dataset type\"\"\"\n",
        "        try:\n",
        "            image = sample['image']\n",
        "            if not isinstance(image, Image.Image):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "\n",
        "            if dataset_name == \"DocVQA\":\n",
        "                question = sample['query']['en'] if isinstance(sample['query'], dict) else sample['query']\n",
        "                answer = sample.get('answers', [''])[0] if 'answers' in sample else ''\n",
        "\n",
        "                prompt = f\"\"\"<|user|>\n",
        "    <|image_1|>\n",
        "    Read the document and answer: {question}<|end|>\n",
        "    <|assistant|>\n",
        "    {answer}<|end|>\"\"\"\n",
        "\n",
        "            elif dataset_name == \"ScienceQA\":\n",
        "                question = sample['question']\n",
        "                choices = sample['choices']\n",
        "                answer_idx = sample['answer']\n",
        "                answer = str(answer_idx)\n",
        "\n",
        "                choices_text = \"\\n\".join([f\"{i}. {choice}\" for i, choice in enumerate(choices)])\n",
        "                prompt = f\"\"\"<|user|>\n",
        "    <|image_1|>\n",
        "    Question: {question}\n",
        "    Choices:\n",
        "    {choices_text}\n",
        "    Answer (0-{len(choices)-1}):<|end|>\n",
        "    <|assistant|>\n",
        "    {answer}<|end|>\"\"\"\n",
        "\n",
        "            elif dataset_name == \"COCO\":\n",
        "                captions = sample.get('sentences', {}).get('raw', [''])\n",
        "                caption = captions[0] if captions else ''\n",
        "\n",
        "                prompt = f\"\"\"<|user|>\n",
        "    <|image_1|>\n",
        "    Describe this image:<|end|>\n",
        "    <|assistant|>\n",
        "    {caption}<|end|>\"\"\"\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = processor(\n",
        "                text=prompt,\n",
        "                images=image,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # Create labels\n",
        "            inputs['labels'] = inputs['input_ids'].clone()\n",
        "\n",
        "            # pad token\n",
        "            pad_id = processor.tokenizer.pad_token_id\n",
        "            if pad_id is None:\n",
        "                pad_id = processor.tokenizer.eos_token_id\n",
        "                processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "\n",
        "            # ─────────────────────────────────────────────\n",
        "            # FIX 1: Replace invalid negative IDs\n",
        "            # ─────────────────────────────────────────────\n",
        "            input_ids = inputs['input_ids']\n",
        "            input_ids[input_ids < 0] = pad_id\n",
        "\n",
        "            # ─────────────────────────────────────────────\n",
        "            # FIX 2: Handle IDs >= embedding vocab size\n",
        "            # use UNK token to safely clip\n",
        "            # ─────────────────────────────────────────────\n",
        "            actual_vocab_size = len(processor.tokenizer)\n",
        "            unk_id = processor.tokenizer.unk_token_id\n",
        "\n",
        "            # clamp overflow tokens\n",
        "            input_ids[input_ids >= actual_vocab_size] = unk_id\n",
        "\n",
        "            # Write back\n",
        "            inputs['input_ids'] = input_ids\n",
        "\n",
        "            # Update labels (mask padding)\n",
        "            attention = inputs['attention_mask']\n",
        "            labels = inputs['labels']\n",
        "            labels = labels.masked_fill(attention == 0, -100)\n",
        "            inputs['labels'] = labels\n",
        "\n",
        "            # Final sanity check\n",
        "            min_id = int(inputs['input_ids'].min().item())\n",
        "            max_id = int(inputs['input_ids'].max().item())\n",
        "\n",
        "            if min_id < 0 or max_id >= actual_vocab_size:\n",
        "                raise ValueError(\n",
        "                    f\"Token ids out of range after fix: min={min_id}, max={max_id}, actual_vocab_size={actual_vocab_size}\"\n",
        "                )\n",
        "\n",
        "            return inputs, (answer if dataset_name != \"COCO\" else caption)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing sample: {e}\")\n",
        "            return None, None\n"
      ],
      "metadata": {
        "id": "NvD9l6kBV3xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 6: Metrics Calculator (Simplified from original)\n",
        "# ============================================================================\n",
        "\n",
        "class MetricsCalculator:\n",
        "    \"\"\"Calculate evaluation metrics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smoothing = SmoothingFunction().method1\n",
        "\n",
        "    def calculate_bleu(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate BLEU-4 score\"\"\"\n",
        "        scores = []\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            pred_tokens = pred.lower().split()\n",
        "            ref_tokens = [ref.lower().split()]\n",
        "            score = sentence_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                                smoothing_function=self.smoothing)\n",
        "            scores.append(score)\n",
        "        return np.mean(scores) * 100\n",
        "\n",
        "    def calculate_rouge(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"Calculate ROUGE scores\"\"\"\n",
        "        rouge_scores = defaultdict(list)\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            scores = self.rouge_scorer.score(ref, pred)\n",
        "            for key, value in scores.items():\n",
        "                rouge_scores[f'{key}_f1'].append(value.fmeasure * 100)\n",
        "        return {k: np.mean(v) for k, v in rouge_scores.items()}\n",
        "\n",
        "    def calculate_exact_match(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate exact match accuracy\"\"\"\n",
        "        matches = sum(1 for pred, ref in zip(predictions, references)\n",
        "                     if pred.strip().lower() == ref.strip().lower())\n",
        "        return (matches / len(predictions)) * 100 if predictions else 0.0\n",
        "\n",
        "    def calculate_all_metrics(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"Calculate all metrics\"\"\"\n",
        "        metrics = {\n",
        "            'bleu4': self.calculate_bleu(predictions, references),\n",
        "            'exact_match': self.calculate_exact_match(predictions, references)\n",
        "        }\n",
        "        metrics.update(self.calculate_rouge(predictions, references))\n",
        "        return metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "MwcfGmMOWD5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_batch_for_forward(batch: Dict[str, torch.Tensor], processor) -> None:\n",
        "    \"\"\"\n",
        "    Basic sanity checks for tensors that commonly cause CUDA asserts:\n",
        "      - shapes of input_ids and attention_mask match\n",
        "      - labels are within [ -100, vocab_size-1 ]\n",
        "      - no negative/too-large token ids\n",
        "    Raises ValueError with descriptive message if something is wrong.\n",
        "    \"\"\"\n",
        "    vocab_size = len(processor.tokenizer)\n",
        "    device_info = {k: (v.device if torch.is_tensor(v) else None) for k, v in batch.items()}\n",
        "\n",
        "    # shape checks\n",
        "    if 'input_ids' in batch and 'attention_mask' in batch:\n",
        "        if batch['input_ids'].shape != batch['attention_mask'].shape:\n",
        "            raise ValueError(f\"Shape mismatch: input_ids {batch['input_ids'].shape} vs attention_mask {batch['attention_mask'].shape}\")\n",
        "\n",
        "    # token id ranges\n",
        "    if 'input_ids' in batch:\n",
        "        min_id = int(batch['input_ids'].min().cpu().item())\n",
        "        max_id = int(batch['input_ids'].max().cpu().item())\n",
        "        if min_id < 0 or max_id >= vocab_size:\n",
        "            raise ValueError(f\"input_ids out of range: min={min_id}, max={max_id}, vocab_size={vocab_size}\")\n",
        "\n",
        "    # labels (allow -100 for ignored positions)\n",
        "    if 'labels' in batch:\n",
        "        labels = batch['labels']\n",
        "        # check -100 allowed\n",
        "        min_label = int(labels.min().cpu().item())\n",
        "        max_label = int(labels.max().cpu().item())\n",
        "        if min_label < -100 or max_label >= vocab_size:\n",
        "            raise ValueError(f\"labels out of range: min={min_label}, max={max_label}, vocab_size={vocab_size}\")\n",
        "    print(\"Batch devices:\", device_info)\n"
      ],
      "metadata": {
        "id": "i8SziWQfZlfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 7: Trainer Class for Feature Adapters\n",
        "# ============================================================================\n",
        "\n",
        "class FeatureAdapterTrainer:\n",
        "    \"\"\"Trainer for feature-based fine-tuning\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Phi3WithFeatureAdapters,\n",
        "        processor,\n",
        "        config: FineTuningConfig,\n",
        "        train_dataset,\n",
        "        eval_dataset,\n",
        "        dataset_name: str\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.config = config\n",
        "        self.train_dataset = train_dataset\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.dataset_name = dataset_name\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "        # Training state\n",
        "        self.global_step = 0\n",
        "        self.best_eval_loss = float('inf')\n",
        "        self.training_history = []\n",
        "\n",
        "        # Setup optimizer\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            [p for p in model.parameters() if p.requires_grad],\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Setup scheduler\n",
        "        total_steps = (len(train_dataset) // config.batch_size) * config.num_epochs\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=config.warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "        os.makedirs(config.checkpoints_dir, exist_ok=True)\n",
        "        os.makedirs(config.visualizations_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"\\n✓ Trainer initialized\")\n",
        "        print(f\"  - Total training steps: {total_steps}\")\n",
        "        print(f\"  - Warmup steps: {config.warmup_steps}\")\n",
        "        print(f\"  - Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
        "\n",
        "    def train_epoch(self, epoch: int):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(range(len(self.train_dataset)), desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "        for idx in range(0, len(self.train_dataset), self.config.batch_size):\n",
        "            # Get batch\n",
        "            batch_samples = []\n",
        "            for i in range(idx, min(idx + self.config.batch_size, len(self.train_dataset))):\n",
        "                inputs, _ = DatasetHandler.prepare_sample(\n",
        "                    self.train_dataset[i],\n",
        "                    self.processor,\n",
        "                    self.dataset_name\n",
        "                )\n",
        "                if inputs is not None:\n",
        "                    batch_samples.append(inputs)\n",
        "\n",
        "            if not batch_samples:\n",
        "                continue\n",
        "\n",
        "            # Collate batch\n",
        "            batch = self._collate_batch(batch_samples)\n",
        "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "\n",
        "            # ---- defensive check before forward to catch issues on CPU/early ----\n",
        "            try:\n",
        "                # perform validation on CPU tensors before moving to cuda (optional)\n",
        "                # If processor lives on CPU, validate on CPU copy - but we've already moved to device.\n",
        "                # If you want validation strictly on CPU, call validate before .to(device)\n",
        "                validate_batch_for_forward(batch, self.processor)\n",
        "            except Exception as ve:\n",
        "                print(\"Batch validation failed:\", ve)\n",
        "                # skip this batch (or raise) to avoid CUDA assert\n",
        "                continue\n",
        "\n",
        "            # Forward pass (now safer)\n",
        "            outputs = self.model(**batch)\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Normalize loss\n",
        "            loss = loss / self.config.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Update weights\n",
        "            if (num_batches + 1) % self.config.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in self.model.parameters() if p.requires_grad],\n",
        "                    self.config.max_grad_norm\n",
        "                )\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.global_step += 1\n",
        "\n",
        "                # Logging\n",
        "                if self.global_step % self.config.logging_steps == 0:\n",
        "                    lr = self.scheduler.get_last_lr()[0]\n",
        "                    print(f\"\\nStep {self.global_step} | Loss: {loss.item():.4f} | LR: {lr:.2e}\")\n",
        "\n",
        "            num_batches += 1\n",
        "            progress_bar.update(self.config.batch_size)\n",
        "\n",
        "            # Periodic evaluation\n",
        "            if self.global_step % self.config.eval_steps == 0:\n",
        "                eval_metrics = self.evaluate()\n",
        "                self.training_history.append({\n",
        "                    'step': self.global_step,\n",
        "                    'epoch': epoch,\n",
        "                    'train_loss': epoch_loss / max(num_batches, 1),\n",
        "                    **eval_metrics\n",
        "                })\n",
        "                self.model.train()\n",
        "\n",
        "            # Save checkpoint\n",
        "            if self.global_step % self.config.save_steps == 0:\n",
        "                self.save_checkpoint(f\"checkpoint_step_{self.global_step}\")\n",
        "\n",
        "            # Clear memory\n",
        "            del batch, outputs, loss\n",
        "            if num_batches % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        progress_bar.close()\n",
        "        avg_loss = epoch_loss / max(num_batches, 1)\n",
        "        print(f\"\\nEpoch {epoch+1} completed | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def _collate_batch(self, batch_samples):\n",
        "        \"\"\"Collate samples into batch\"\"\"\n",
        "        # Simple collation - pad to max length in batch\n",
        "        max_length = max(s['input_ids'].shape[1] for s in batch_samples)\n",
        "\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        pixel_values = []\n",
        "        labels = []\n",
        "\n",
        "        for sample in batch_samples:\n",
        "            # Pad input_ids\n",
        "            pad_length = max_length - sample['input_ids'].shape[1]\n",
        "            input_ids.append(F.pad(sample['input_ids'], (0, pad_length), value=self.processor.tokenizer.pad_token_id))\n",
        "            attention_mask.append(F.pad(sample['attention_mask'], (0, pad_length), value=0))\n",
        "            labels.append(F.pad(sample['labels'], (0, pad_length), value=-100))\n",
        "            pixel_values.append(sample['pixel_values'])\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.cat(input_ids, dim=0),\n",
        "            'attention_mask': torch.cat(attention_mask, dim=0),\n",
        "            'pixel_values': torch.cat(pixel_values, dim=0),\n",
        "            'labels': torch.cat(labels, dim=0)\n",
        "        }\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate model\"\"\"\n",
        "        self.model.eval()\n",
        "        eval_loss = 0\n",
        "        predictions = []\n",
        "        references = []\n",
        "\n",
        "        print(\"\\nRunning evaluation...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx in tqdm(range(len(self.eval_dataset)), desc=\"Evaluating\"):\n",
        "                inputs, reference = DatasetHandler.prepare_sample(\n",
        "                    self.eval_dataset[idx],\n",
        "                    self.processor,\n",
        "                    self.dataset_name\n",
        "                )\n",
        "\n",
        "                if inputs is None:\n",
        "                    continue\n",
        "\n",
        "                # Move to device\n",
        "                inputs_eval = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Calculate loss\n",
        "                outputs = self.model(**inputs_eval)\n",
        "                eval_loss += outputs.loss.item()\n",
        "\n",
        "                # Generate prediction\n",
        "                gen_inputs = {k: v for k, v in inputs_eval.items() if k != 'labels'}\n",
        "                pred_ids = self.model.generate(**gen_inputs, max_new_tokens=100)\n",
        "                pred_text = self.processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "                if \"<|assistant|>\" in pred_text:\n",
        "                    pred_text = pred_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "                predictions.append(pred_text)\n",
        "                references.append(reference)\n",
        "\n",
        "                if idx % 10 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics_calc = MetricsCalculator()\n",
        "        metrics = metrics_calc.calculate_all_metrics(predictions, references)\n",
        "        metrics['eval_loss'] = eval_loss / len(self.eval_dataset)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(\"Evaluation Results:\")\n",
        "        print(f\"  Loss: {metrics['eval_loss']:.4f}\")\n",
        "        print(f\"  BLEU-4: {metrics['bleu4']:.2f}\")\n",
        "        print(f\"  ROUGE-L: {metrics['rougeL_f1']:.2f}\")\n",
        "        print(f\"  Exact Match: {metrics['exact_match']:.2f}%\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STARTING FEATURE-BASED FINE-TUNING\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            print(f\"\\n--- Epoch {epoch + 1}/{self.config.num_epochs} ---\")\n",
        "            epoch_loss = self.train_epoch(epoch)\n",
        "\n",
        "            # Epoch-end evaluation\n",
        "            eval_metrics = self.evaluate()\n",
        "\n",
        "            # Save best model\n",
        "            if eval_metrics['eval_loss'] < self.best_eval_loss:\n",
        "                self.best_eval_loss = eval_metrics['eval_loss']\n",
        "                self.save_checkpoint(\"best_model\")\n",
        "                print(f\"✓ New best model saved (loss: {self.best_eval_loss:.4f})\")\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"TRAINING COMPLETED\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Save final model\n",
        "        self.save_checkpoint(\"final_model\")\n",
        "\n",
        "        return self.training_history\n",
        "\n",
        "    def save_checkpoint(self, name: str):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        checkpoint_path = os.path.join(self.config.checkpoints_dir, f\"{name}.pt\")\n",
        "        self.model.save_adapters(checkpoint_path)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: Evaluation Pipeline (Pre vs Post Fine-tuning)\n",
        "# ============================================================================\n",
        "\n",
        "class ComparisonEvaluator:\n",
        "    \"\"\"Evaluate and compare pre-trained vs fine-tuned models\"\"\"\n",
        "\n",
        "    def __init__(self, processor, config: FineTuningConfig):\n",
        "        self.processor = processor\n",
        "        self.config = config\n",
        "        self.metrics_calc = MetricsCalculator()\n",
        "\n",
        "    def evaluate_model(self, model, dataset, dataset_name: str, model_type: str):\n",
        "        \"\"\"Evaluate a model on dataset\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Evaluating {model_type} model on {dataset_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        references = []\n",
        "        inference_times = []\n",
        "\n",
        "        max_samples = self.config.max_samples_eval or len(dataset)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx in tqdm(range(min(max_samples, len(dataset))), desc=f\"Evaluating {model_type}\"):\n",
        "                inputs, reference = DatasetHandler.prepare_sample(\n",
        "                    dataset[idx],\n",
        "                    self.processor,\n",
        "                    dataset_name\n",
        "                )\n",
        "\n",
        "                if inputs is None:\n",
        "                    continue\n",
        "\n",
        "                # Move to device\n",
        "                device = next(model.parameters()).device\n",
        "                gen_inputs = {k: v.to(device) for k, v in inputs.items() if k != 'labels'}\n",
        "\n",
        "                # Measure inference time\n",
        "                start_time = time.time()\n",
        "                pred_ids = model.generate(**gen_inputs, max_new_tokens=100, do_sample=False)\n",
        "                inference_time = time.time() - start_time\n",
        "\n",
        "                pred_text = self.processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
        "                if \"<|assistant|>\" in pred_text:\n",
        "                    pred_text = pred_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "                predictions.append(pred_text)\n",
        "                references.append(reference)\n",
        "                inference_times.append(inference_time)\n",
        "\n",
        "                if idx % 10 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = self.metrics_calc.calculate_all_metrics(predictions, references)\n",
        "        metrics['avg_inference_time'] = np.mean(inference_times)\n",
        "\n",
        "        print(f\"\\nResults for {model_type}:\")\n",
        "        print(f\"  BLEU-4: {metrics['bleu4']:.2f}\")\n",
        "        print(f\"  ROUGE-1: {metrics['rouge1_f1']:.2f}\")\n",
        "        print(f\"  ROUGE-L: {metrics['rougeL_f1']:.2f}\")\n",
        "        print(f\"  Exact Match: {metrics['exact_match']:.2f}%\")\n",
        "        print(f\"  Avg Inference Time: {metrics['avg_inference_time']:.4f} s\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 9: Utility functions and example script\n",
        "# ============================================================================\n",
        "\n",
        "def print_model_summary(model: nn.Module):\n",
        "    \"\"\"Print simple summary of model adapters and parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nModel summary:\")\n",
        "    print(f\"  - Total params: {total_params:,}\")\n",
        "    print(f\"  - Trainable params: {trainable:,}\")\n",
        "    print(f\"  - Frozen params: {total_params - trainable:,}\\n\")\n",
        "\n",
        "\n",
        "def safe_device():\n",
        "    \"\"\"Return appropriate device (cpu if no cuda)\"\"\"\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J8LVQ7kwWJZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqeXDLM5UzBB"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # NOTE: This example is intentionally conservative. Running the full training\n",
        "    # loop requires GPU and enough memory. Use these calls as a template.\n",
        "    try:\n",
        "        # Configs\n",
        "        model_cfg = ModelConfig()\n",
        "        ft_cfg = FineTuningConfig()\n",
        "        # The explicit setting `model_cfg.use_quantization = False` caused the model\n",
        "        # to be too large for the GPU. Removing this line will re-enable 4-bit\n",
        "        # quantization by default, allowing the model to fit on the GPU.\n",
        "\n",
        "        # Load model & processor\n",
        "        model_base, processor = Phi3VisionLoader.load_model_and_processor(model_cfg)\n",
        "\n",
        "        # Wrap with adapters\n",
        "        model_with_adapters = Phi3WithFeatureAdapters(model_base, ft_cfg)\n",
        "\n",
        "        # Move adapters-only model to appropriate device if base model not already there\n",
        "        device = safe_device()\n",
        "        model_with_adapters.to(device)\n",
        "\n",
        "        print_model_summary(model_with_adapters)\n",
        "\n",
        "        # Load small subsets for quick smoke test (keep counts tiny to avoid long runs)\n",
        "        train_ds = DatasetHandler.load_training_dataset(dataset_name=\"DocVQA\", max_samples=ft_cfg.max_samples_train or 8)\n",
        "        eval_ds = DatasetHandler.load_training_dataset(dataset_name=\"DocVQA\", max_samples=ft_cfg.max_samples_eval or 4)\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = FeatureAdapterTrainer(\n",
        "            model=model_with_adapters,\n",
        "            processor=processor,\n",
        "            config=ft_cfg,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=eval_ds,\n",
        "            dataset_name=\"DocVQA\"\n",
        "        )\n",
        "        tokenizer = processor.tokenizer\n",
        "        model_base.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        # WARNING: The full `trainer.train()` may be slow/expensive. Uncomment to run.\n",
        "        history = trainer.train()\n",
        "\n",
        "        # Instead, run a quick evaluation of the pretrained wrapped model (no training)\n",
        "        comparator = ComparisonEvaluator(processor, ft_cfg)\n",
        "        print(\"\\nRunning quick pre-finetune evaluation (wrapped model with frozen base)...\")\n",
        "        pre_metrics = comparator.evaluate_model(model_with_adapters, eval_ds, \"DocVQA\", model_type=\"wrapped_pretrained\")\n",
        "\n",
        "        # If you have a saved adapter checkpoint, you can load & evaluate the fine-tuned adapters:\n",
        "        # adapter_checkpoint = \"./phi3_finetuning_outputs/checkpoints/best_model.pt\"\n",
        "        # model_with_adapters.load_adapters(adapter_checkpoint)\n",
        "        # post_metrics = comparator.evaluate_model(model_with_adapters, eval_ds, \"DocVQA\", model_type=\"wrapped_finetuned\")\n",
        "\n",
        "        print(\"\\nScript completed. To run training, uncomment `trainer.train()` in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred in the example entrypoint: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "QeJWJS0WWp5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9E5nWj9bOKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}