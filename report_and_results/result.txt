Calculating metrics for ScienceQA...

============================================================
Metrics Summary for ScienceQA
============================================================
BLEU-4: 0.20
ROUGE-1 F1: 3.81
ROUGE-L F1: 3.81
Exact Match: 0.00%
Numeric Accuracy: 10.00%
Avg Inference Time: 314.434s
============================================================

Results saved to: ./phi3_vision_outputs/scienceqa_replication.json

============================================================
EXPERIMENT 2: Domain Adaptation (Document Understanding)
============================================================
Loading DocVQA dataset...
Loaded 10 samples from DocVQA

============================================================
Evaluating on DocVQA
============================================================
Processingâ€‡DocVQA:â€‡100%
â€‡10/10â€‡[1:02:15<00:00,â€‡373.32s/it]

Calculating metrics for DocVQA...

============================================================
Metrics Summary for DocVQA
============================================================
BLEU-4: 0.42
ROUGE-1 F1: 4.06
ROUGE-L F1: 4.06
Exact Match: 0.00%
Numeric Accuracy: 0.00%
Avg Inference Time: 373.433s
============================================================

Results saved to: ./phi3_vision_outputs/docvqa_zero_shot.json

============================================================
EXPERIMENT 3: Tuning Strategy Comparison
============================================================

Note: Full fine-tuning implementation requires training loop.
For Colab, recommend using HuggingFace Trainer or custom training loop
with gradient accumulation to handle memory constraints.
Setting up LoRA configuration...
trainable params: 7,602,176 || all params: 4,154,223,616 || trainable%: 0.1830

LoRA model ready for fine-tuning!

============================================================
GENERATING VISUALIZATIONS
============================================================
Saved metrics comparison to: ./phi3_vision_outputs/visualizations/metrics_comparison.png
Saved metrics heatmap to: ./phi3_vision_outputs/visualizations/metrics_heatmap.png
/tmp/ipython-input-2487915939.py:93: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.
  axes[0].boxplot(times, labels=datasets)
Saved inference time plot to: ./phi3_vision_outputs/visualizations/inference_time.png
Saved sample predictions to: ./phi3_vision_outputs/visualizations/sample_predictions.png
Saved sample predictions to: ./phi3_vision_outputs/visualizations/sample_predictions.png

âœ“ All visualizations generated!

============================================================
GENERATING SUMMARY REPORT
============================================================

Saved evaluation report to: ./phi3_vision_outputs/visualizations/evaluation_report.txt

======================================================================
PHI-3 VISION EVALUATION SUMMARY REPORT
======================================================================

Model: microsoft/Phi-3-vision-128k-instruct
Quantization: 4-bit
Total Samples Evaluated: 20
Evaluation Date: 2025-11-24 12:20:31

======================================================================
METRICS BY DATASET
======================================================================


ScienceQA:
----------------------------------------
  BLEU Scores:
    bleu1: 1.08
    bleu2: 0.35
    bleu3: 0.25
    bleu4: 0.20
  ROUGE-F1 Scores:
    rouge1_f1: 3.81
    rouge2_f1: 0.00
    rougeL_f1: 3.81
  Other Metrics:
    exact_match: 0.00
    numeric_accuracy: 10.00
  Inference Time:
    Mean: 314.434s
    Median: 304.172s
    Std: 64.002s

DocVQA:
----------------------------------------
  BLEU Scores:
    bleu1: 2.25
    bleu2: 0.71
    bleu3: 0.52
    bleu4: 0.42
  ROUGE-F1 Scores:
    rouge1_f1: 4.06
    rouge2_f1: 0.00
    rougeL_f1: 4.06
  Other Metrics:
    exact_match: 0.00
    numeric_accuracy: 0.00
  Inference Time:
    Mean: 373.433s
    Median: 373.444s
    Std: 0.695s

======================================================================
END OF REPORT
======================================================================

======================================================================
PIPELINE COMPLETE
======================================================================

Results saved to: ./phi3_vision_outputs
Visualizations saved to: ./phi3_vision_outputs/visualizations

======================================================================
KEY FINDINGS SUMMARY
======================================================================

ðŸ“Š Best Performance: DocVQA (BLEU-4: 0.42)
ðŸ“Š Worst Performance: ScienceQA (BLEU-4: 0.20)

âš¡ Fastest Inference: ScienceQA (314.434s per sample)

======================================================================
